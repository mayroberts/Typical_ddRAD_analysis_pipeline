# 1. ddRADseq Data Processing

## Wee background on the data:
Samples were collected on both San Clemente and Catalina Islands or grown in the greenhouse from seeds collected there. We also have outgroup samples from herbarium specimens for the phylogenetic component. We'll have another plate of data coming in with additional samples from those islands as well as the out groups from the mainland. The data is SE150 and already demultiplexed by the sequencing facility (UCR Genomics Core).  

## Things to know before you start:
### Where's the data:
We store our raw data/back up in the Genetics-Synology located at: `/Volumes/Genetics-Synology/` / which can also be found on the desktop of the Mac.  This data (`.fastqz` files) is copied to the project folder in Projects on the desktop. This is the folder/directory where all output files, from ipyrad and downstream analysis with other software, are written to. 

# Table of Contents
- [Data processing with ipyrad - setting up parameters file](#data-processing-with-ipyrad)
- 	 [Step 1: Demultiplexing (even when you have demultiplexed data)](#step-1-demultiplexing-even-when-you-have-demultiplexed-data-4min)
- 	 [Step 2: Filter reads](#step-2-filter-reads-8min)
- 	 [Step 3: Clustering within Samples](#step-3-clustering-within-samples-26hrs)
- 	 [Step 4: Estimation of heterozygosity and sequencing error rate](#step-4-estimation-of-heterozygosity-and-sequencing-error-rate-6min)
- 	 [Step 5: Consensus base calling](#step-5-consensus-base-calling-50min-1.5hrs)
- 	 [Step 6: Cluster across samples](#step-6-cluster-across-samples)
- 	 [Step 7a: Filter and write output files](#step-7a-filter-and-write-output-files-2min)
- 	 [Step 7b: Filter out sample files by branching](#step7b-filter-out-sample-files-by-branching)
- 	 [Step 7c: Adjusting filter parameters for a whole new set of outfiles - by branching again](#step7c-adjusting-filter-parameters-for-a-whole-new-set-of-outfiles-by-branching-again)

  
## Data processing with iPyrad
The ipyrad tutorial we follow is here: https://ipyrad.readthedocs.io/en/master/tutorial_intro_cli.html

For all of the 7 steps in ipyrad, it produces a stats file for results from that stage. You can either see the stats file in terminal using the command `cat /PATH/FILENAME` or `ipyrad -p params-TAXA.txt -r`
We get ipyrad software going by opening terminal, and "turning on" ipyrad by using the command: 

            conda activate IPYRAD2024 
            
Then we create a new ipyrad parameter file in the current directory (I have it in the project directory) with the commmand `ipyrad -n [name of project or species]` so: 

            ipyrad -n nameoftaxa #eg:"SIFI" 
* the first run I only used plate 1 and 2 of data so named the file/directory SIFI_1_2 to hopefully keep output files distinct later on when I re analyze with all three plates of data.
* SIFI_pl123a refers to data builds using data from all 3 plates though plate 3 will be re-run again by UCR due to poor quality seqs

This produces params file with developer defaults. This is what we changed them to.  

           ------- ipyrad params file (v.0.9.96)-------------------------------------------
            name_of_your_choice            ## [0] [assembly_name]: Assembly name. Used to name output directories for assembly steps
            /Users/kristenlehman/Desktop/Projects/SIFI/ipyrad_data_processing/SIFI_123a ## [1] [project_dir]: Project dir (made in curdir if not present)
                                           ## [2] [raw_fastq_path]: Location of raw non-demultiplexed fastq files
                                           ## [3] [barcodes_path]: Location of barcodes file
            /Users/kristenlehman/Desktop/Projects/SIFI/raw_data_pl123/*.fastq.gz ## [4] [sorted_fastq_path]: Location of demultiplexed/sorted fastq files
            denovo                         ## [5] [assembly_method]: Assembly method (denovo, reference)
                                           ## [6] [reference_sequence]: Location of reference sequence file
            ddrad                          ## [7] [datatype]: Datatype (see docs): rad, gbs, ddrad, etc.
            AATT, TAA                      ## [8] [restriction_overhang]: Restriction overhang (cut1,) or (cut1, cut2)
            5                              ## [9] [max_low_qual_bases]: Max low quality base calls (Q<20) in a read
            33                             ## [10] [phred_Qscore_offset]: phred Q score offset (33 is default and very standard)
            6                              ## [11] [mindepth_statistical]: Min depth for statistical base calling
            6                              ## [12] [mindepth_majrule]: Min depth for majority-rule base calling
            10000                          ## [13] [maxdepth]: Max cluster depth within samples
            0.85                           ## [14] [clust_threshold]: Clustering threshold for de novo assembly
            0                              ## [15] [max_barcode_mismatch]: Max number of allowable mismatches in barcodes
            2                              ## [16] [filter_adapters]: Filter for adapters/primers (1 or 2=stricter)
            35                             ## [17] [filter_min_trim_len]: Min length of reads after adapter trim
            2                              ## [18] [max_alleles_consens]: Max alleles per site in consensus sequences
            0.05                           ## [19] [max_Ns_consens]: Max N's (uncalled bases) in consensus
            0.25                           ## [20] [max_Hs_consens]: Max Hs (heterozygotes) in consensus #default 0.05/had 0.25#
            50                             ## [21] [min_samples_locus]: Min # samples per locus for output #default 4/had 50#
            .2                             ## [22] [max_SNPs_locus]: Max # SNPs per locus
            5                              ## [23] [max_Indels_locus]: Max # of indels per locus #default 8/had 5
            0.25                           ## [24] [max_shared_Hs_locus]: Max # heterozygous sites per locus #default .5/had 0.25
            0, 0, 0, 0                     ## [25] [trim_reads]: Trim raw read edges (R1>, <R1, R2>, <R2) (see docs)
            0, 0, 0, 0                     ## [26] [trim_loci]: Trim locus edges (see docs) (R1>, <R1, R2>, <R2)
            G, a, g, k, m, l, n, p, s, u, t, v ## [27] [output_formats]: Output formats (see docs)
                                           ## [28] [pop_assign_file]: Path to population assignment file
                                           ## [29] [reference_as_filter]: Reads mapped to this reference are removed in step 3

You can also of course copy (`cp FILENAME PATH/TO/LOC/OR/NEW_FILENAME`) a params file from another project and change the necessary parameters as needed. \
Check out this page for more explanation on each parameter: https://ipyrad.readthedocs.io/en/master/6-params.html \
* Also, it turns out that [22] Max_SNP_locus value should actually be a 0-1 as a percentage of SNPs allowed per locus. Default is 0.2.
                                           
## Step 1: Demultiplexing (even when you have demultiplexed data) (4min)
Eventhough our data is demultiplexed (organized into individual files based on barcodes) we run Step 1, this step creates the `.json` file necessary for the filtering step. This step takes <4 min with 2 plates of 150bpSE data

            ipyrad -p params-TAXA.txt -s 1

Take a look at the s1_demultiplex_stats.txt file that it produced. It will show your files and how many total raw seqs per sample.

## Step 2: Filter reads (~8min)
Here ipyrad filters reads based on quality scores.

            ipyrad -p params-TAXA.txt -s 2

This step produces a directory of filtered files `TAXA_edits`. In this directory, there is a file s2_rawedit_stats.txt which summarizes # of sequences filtered from each file and reason.

## Step 3: Clustering within Samples (26hrs)
Here, ipyrad first de-replicates and then identifies all reads within a sample that map to the same locus. This step takes several hours (?) started run at ~11am 7.1024

            ipyrad -p params-TAXA.txt -s 3

The output file of aligned clusters are in the directory `TAXA_clust_0.85`. If you want to see what the files of aligned clusters look like, use `gunzip` to unzip the file and `head -n 30` to show the top 30 lines of the file.  

            gunzip -c TAXA_clust_0.85/1A_0.clustS.gz | head -n 30


Again, like all steps in ipyrad, there is a stats file which is good to check out to get an idea of the quality of data for each sample or simply use the command:

            ipyrad -p params-TAXA.txt -r

## Step 4: Estimation of heterozygosity and sequencing error rate (6min)
"We need to know which reads are “real” because in diploid organisms there are a maximum of 2 alleles at any given locus. If we look at the raw data and there are 5 or ten different “alleles”, and 2 of them are very high frequency, and the rest are singletons then this gives us evidence that the 2 high frequency alleles are good reads and the rest are probably not."

            ipyrad -p params-TAXA.txt -s 4

There are no new output files produced at this step. But check the `ipyrad -p params-TAXA.txt -r` to see the estimated heterozygosity and error rates per individual.

## Step 5: Consensus base calling (50min - 1.5hrs)
"Step 5 uses the inferred error rate and heterozygosity to call the consensus of sequences within each cluster. Here we are identifying what we believe to be the real haplotypes at each locus within each sample."

            ipyrad -p params-TAXA.txt -s 5

This step creats a new directory called `TAXA_consens` which stores consensus sequences for each loci for each sample. 
In the stats produced from this step we can see the number of good reads in the colum `reads_consens` 

## Step 6: Cluster across samples 
Here we use the consens sequences for each samples haplotype, ipyrad now identifies similar sequences at each locus across samples - effectively identifies the loci we have sequences for from all samples (?).

            ipyrad -p params-TAXA.txt -s 6

## Step 7a: Filter and write output files (2min)

The filters in this step are for max # of indels per locus, max heterozygosity, max snps per locus and min number of samples per locus. The threshholds for these filters can all be adjusted by changing the appropriate lines in the `params-TAXA.txt` file. Finally, after filtering, ipyrad writes output files as any/all file formats into the `TAXA_outfiles` directory.

            ipyrad -p params-TAXA.txt -s 7

The key output file here, is the .vcf file. A vcf file is a tab-delimited text file that stores and reports genomic sequence variations and read depth values for each genomic position in a group of sequenced samples.  With these output files, you are ready to start analyzing!...

## Step7b: Filter out sample files by branching

If you want to filter by something other than those included in the params file, then this is where we Branch (https://ipyrad.readthedocs.io/en/master/8-branching.html). For the first run of plate 3 which was a pretty poor run, I ended up needing to remove some samples from plates 1 and 2 and many from plate 3 based on low consensus reads.

First make a .txt file of "to remove" list of samples - I called mine `SIFI123a_filter_list_less_than_10k.txt` to indicate how I decided which samples to remove (<10k consensus reads - this was arbitrary). You can also make A to keep list if that's easier. In this case, you would not have a `-` before your to keep .txt file or list. This 'to remove list' needs to be in the same directory as the old params file. Before I knew better I just made a list of the sample names I wanted to remove, but apparently, if it's easier you can keep all the stats after the name if you are taking the name from the stats file. 

In this line to create a new branch/version of outfiles, `- p` indicates the orginal params file; `-b` indicates your new name for the files created by this branch/iteration; `-` (ie minus) indicates that you want to remove the list of samples found in the .txt file after the `-`.

            ipyrad -p params-SIFI.txt -b SIFI-less10kconreads - SIFI123a_filter_list_less_than_10k.txt
            
This makes a new branch with a new params file called params-SIFI-less10kconreads.txt as seen in the message here:

                          loading Assembly: SIFI_pl123a
                          from saved path: ~/Desktop/Projects/SIFI/ipyrad_data_processing/SIFI_123a/SIFI_pl123a.json
                          dropping 66 samples
                          creating a new branch called 'SIFI-less10kconreads' with 186 Samples
                          writing new params file to params-SIFI-less10kconreads.txt
Check the `ipyrad -p SIFI-less10kconreads.txt -r` to make sure your list looks right.

Then run step 7 using the new params file:

            ipyrad -p params-SIFI-less10kconreads.txt -s 7

You should now have a new directory `SIFI-less10kconreads_outfiles` which has the filtered output files. On to Analysis!...

## Step7c: Adjusting filter parameters for a whole new set of outfiles - by branching again

In the original params file, we had set `## [22] [max_SNPs_locus]: Max # SNPs per locus` to 25. In this next iteration we're going to reduce this to 1 SNP per locus, this will tighten up our results in that we are only taking the single SNP 

Because I still want to remove the bad sample sequences I will include that .txt file but will make sure to rename this new iteration something meaningful.

            ipyrad -p params-SIFI.txt -b SIFI-less10kconreads-maxSNPploc1 - SIFI123a_filter_list_less_than_10k.txt

What this does is create a new params file called `params-SIFI-less10kconreads-maxSNPploc1.txt`. Open it and change the parametes as you wat, [I only changed `## [22] [max_SNPs_locus]: Max # SNPs per locus` from 25 to 1. And then run it. 

            ipyrad -p params-SIFI-less10kconreads-maxSNPploc1.txt -s 7

And then move on to analysis again :)

